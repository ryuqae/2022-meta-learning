{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryuqae/2022-meta-learning/blob/main/Clickbait_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1hAniWQM0Ku",
        "outputId": "46cda93b-2559-48fd-a963-0b9c4e911504"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "==================== \n",
            "Current cuda device  0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "import pandas as pd\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoConfig, AutoModel, AutoTokenizer, Trainer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from pprint import pprint\n",
        "\n",
        "pd.set_option('display.max_colwidth', 1000)\n",
        "pd.set_option('display.max_rows', 1000)\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
        "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
        "                    level=logging.INFO,\n",
        "                    )\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "GPU_NUM = 0\n",
        "\n",
        "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.set_device(device) # change allocation of current GPU\n",
        "    print ('==================== \\nCurrent cuda device ', torch.cuda.current_device()) # check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5t8cGSClM3Wr"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "DATA_DIR = '/content/drive/MyDrive/Meta Learning/kaggle_clickbait'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SJlFjEYur34"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'), index_col='id').dropna()\n",
        "valid = pd.read_csv(os.path.join(DATA_DIR, 'valid.csv'), index_col='id').dropna()\n",
        "test = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'), index_col='id').dropna().reset_index()\n",
        "\n",
        "train = train[train.label!='other'].reset_index()\n",
        "valid = valid[valid.label!='other'].reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EudZoUslNUH-",
        "outputId": "27306828-732e-4889-a693-4c2669c081f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((18330, 4), (2624, 4), (5629, 3))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.shape, valid.shape, test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPZ-o6rd5Nsa",
        "outputId": "bac31d43-55b1-4a68-dcf2-12a11ff1adbc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "news         14606\n",
              "clickbait     3724\n",
              "Name: label, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "CBTvNw5cDaIJ",
        "outputId": "e191fac0-1d7d-4290-b8f1-b038252a74f9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0faed463-6db9-4c88-bebb-6023ee0d7bbb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5562</th>\n",
              "      <td>5589</td>\n",
              "      <td>Want to be neighbors with the Obamas, Ivanka Trump and Jeff Bezos? Here’s what it will cost you.</td>\n",
              "      <td>Want to become neighbors with the Obamas, Ivanka Trump and Jeff Bezos in one of the District’s hottest and most exclusive enclaves, Kalorama, but don’t want the hassle of modernizing a historic home? You are in luck. The French Embassy is selling off part of the land where its ambassador lives for $5.6 million. The 0.58-acre lot at 2221 Kalorama Road is roomy enough to build a grand mansion or up to five homes. It’s the first time in decades a parcel of land with no home on it has been for sale on the open market in this tony neighborhood. “What makes this land so unique is the zoning, R1B, which allows for a plethora of things you can build there,” said Alex Venditti of Coldwell Banker Residential Brokerage, who is co-listing the property with the Morrell-Roth team from Compass. The French Embassy acquired the 1910 Tudor Revival residence in 1936. Additional lots that overlooked Kalorama Circle were purchased in 1941 to expand the parcel to 3.6 acres. This sale will reduce the lot...</td>\n",
              "      <td>clickbait</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14891</th>\n",
              "      <td>15113</td>\n",
              "      <td>Elite donors push Democrats left on climate and immigration, but right on taxes</td>\n",
              "      <td>Bernie Sanders has identified the cancer he thinks is coursing through the Democratic Party’s bloodstream. In speech after speech, the Vermont senator has gone after the party’s donor class — and its hold on politicians — as the central impediment to both a more populist Democratic Party and its electoral success. “I believe strongly that the party must break loose from its corporate establishment ties,” Sanders said in a New York Times shortly after the election. “We must have the courage to take on the greed and power of Wall Street, the drug companies, the insurance companies and the fossil fuel industry. ” Sanders’s argument has a lot going for it. Hillary Clinton campaign’s spent much of the summer fundraising with donors, and ignored the union organizers in the Rust Belt in a way that backfired spectacularly. The WikiLeaks emails revealed that conservative donors like Israeli hawk Haim Saban were closely involved with the Clinton team’s policy shop. Her campaign was certainly...</td>\n",
              "      <td>news</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0faed463-6db9-4c88-bebb-6023ee0d7bbb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0faed463-6db9-4c88-bebb-6023ee0d7bbb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0faed463-6db9-4c88-bebb-6023ee0d7bbb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "          id  \\\n",
              "5562    5589   \n",
              "14891  15113   \n",
              "\n",
              "                                                                                                  title  \\\n",
              "5562   Want to be neighbors with the Obamas, Ivanka Trump and Jeff Bezos? Here’s what it will cost you.   \n",
              "14891                   Elite donors push Democrats left on climate and immigration, but right on taxes   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          text  \\\n",
              "5562   Want to become neighbors with the Obamas, Ivanka Trump and Jeff Bezos in one of the District’s hottest and most exclusive enclaves, Kalorama, but don’t want the hassle of modernizing a historic home? You are in luck. The French Embassy is selling off part of the land where its ambassador lives for $5.6 million. The 0.58-acre lot at 2221 Kalorama Road is roomy enough to build a grand mansion or up to five homes. It’s the first time in decades a parcel of land with no home on it has been for sale on the open market in this tony neighborhood. “What makes this land so unique is the zoning, R1B, which allows for a plethora of things you can build there,” said Alex Venditti of Coldwell Banker Residential Brokerage, who is co-listing the property with the Morrell-Roth team from Compass. The French Embassy acquired the 1910 Tudor Revival residence in 1936. Additional lots that overlooked Kalorama Circle were purchased in 1941 to expand the parcel to 3.6 acres. This sale will reduce the lot...   \n",
              "14891  Bernie Sanders has identified the cancer he thinks is coursing through the Democratic Party’s bloodstream. In speech after speech, the Vermont senator has gone after the party’s donor class — and its hold on politicians — as the central impediment to both a more populist Democratic Party and its electoral success. “I believe strongly that the party must break loose from its corporate establishment ties,” Sanders said in a New York Times shortly after the election. “We must have the courage to take on the greed and power of Wall Street, the drug companies, the insurance companies and the fossil fuel industry. ” Sanders’s argument has a lot going for it. Hillary Clinton campaign’s spent much of the summer fundraising with donors, and ignored the union organizers in the Rust Belt in a way that backfired spectacularly. The WikiLeaks emails revealed that conservative donors like Israeli hawk Haim Saban were closely involved with the Clinton team’s policy shop. Her campaign was certainly...   \n",
              "\n",
              "           label  \n",
              "5562   clickbait  \n",
              "14891       news  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.groupby('label').sample(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSA09sfPpcJj"
      },
      "source": [
        "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xci23dgxCpyg"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxK-1eHfCv1u"
      },
      "outputs": [],
      "source": [
        "a = tokenizer.encode(\"hello this is my name\", max_length = 128, truncation=True, padding=True, return_tensors='pt')\n",
        "b = tokenizer.encode(\"not my name is this\", max_length = 128, truncation=True, padding=True, return_tensors='pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV4NRA56L_bs",
        "outputId": "e3f890be-2d4a-42f1-8c9e-22f1427d9ec3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 101, 7592, 2023, 2003, 2026, 2171,  102],\n",
              "        [ 101, 2025, 2026, 2171, 2003, 2023,  102]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cat((a,b), dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RjyGEnWC7Pc",
        "outputId": "ad88f09f-2cde-46de-bbda-f81449e78b17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 101, 7592, 2023, 2003, 2026, 2171,  102,  101, 2025, 2026, 2171, 2003,\n",
              "         2023,  102]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cat((a, b), dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cp3AJE9HRB98"
      },
      "outputs": [],
      "source": [
        "class ClickbaitDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 headlines:list, \n",
        "                 texts:list,\n",
        "                 concat_mode:str='cosine_similarity',\n",
        "                 labels:list=None, \n",
        "                 label_dict: dict =None,\n",
        "                 max_seq_length: int=512,\n",
        "                 model_name: str='distilbert-base-uncased'):\n",
        "        \n",
        "        self.headlines = headlines\n",
        "        self.texts = texts\n",
        "\n",
        "        self.concat_mode = concat_mode\n",
        "        self.labels = labels\n",
        "        self.label_dict = label_dict\n",
        "\n",
        "        if self.label_dict is None and labels is not None:\n",
        "            # {'clickbait': 0, 'news': 1, 'other': 2}\n",
        "            # using this instead of `sklearn.preprocessing.LabelEncoder`\n",
        "            # no easily handle unknown target values\n",
        "            self.label_dict = dict(zip(sorted(set(labels)), range(len(set(labels)))))\n",
        "\n",
        "        self.max_seq_length=max_seq_length\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        self.sep_vid = self.tokenizer.vocab[\"[SEP]\"]\n",
        "        self.cls_vid = self.tokenizer.vocab[\"[CLS]\"]\n",
        "        self.pad_vid = self.tokenizer.vocab[\"[PAD]\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        x = self.texts[index]\n",
        "        x_head = self.headlines[index]\n",
        "\n",
        "        x_encoded = self.tokenizer.encode(x_head, x, add_special_tokens=True, \n",
        "                                           truncation=True, \n",
        "                                          max_length=self.max_seq_length, \n",
        "                                          return_tensors=\"pt\").squeeze(0)\n",
        "\n",
        "        x_head_encoded = self.tokenizer.encode(x_head, add_special_tokens=True, \n",
        "                                               truncation=True, \n",
        "                                               max_length=self.max_seq_length, \n",
        "                                               return_tensors=\"pt\").squeeze(0)\n",
        "\n",
        "        \n",
        "        x_encoded_length = x_encoded.size(0)\n",
        "        x_head_encoded_length = x_head_encoded.size(0)\n",
        "\n",
        "\n",
        "        # Manually Padding...\n",
        "        pad_size = self.max_seq_length - x_encoded_length\n",
        "        head_pad_size = self.max_seq_length - x_head_encoded_length\n",
        "\n",
        "        pad_ids = torch.Tensor([self.pad_vid] * pad_size).long()\n",
        "        head_pad_ids = torch.Tensor([self.pad_vid] * head_pad_size).long()\n",
        "        \n",
        "        x_tensor = torch.cat((x_encoded, pad_ids))\n",
        "        x_head_tensor = torch.cat((x_head_encoded, head_pad_ids))\n",
        "\n",
        "        x_tensor_agg = torch.cat((x_head_tensor.unsqueeze(0), x_tensor.unsqueeze(0)), dim=0).flatten()\n",
        "\n",
        "        # Manually Masking...\n",
        "        mask = torch.ones_like(x_encoded, dtype=torch.int8)\n",
        "        mask_pad = torch.zeros_like(pad_ids, dtype=torch.int8)\n",
        "        mask = torch.cat((mask, mask_pad),dim=0)\n",
        "\n",
        "        head_mask = torch.ones_like(x_head_encoded, dtype=torch.int8)\n",
        "        head_mask_pad = torch.zeros_like(head_pad_ids, dtype=torch.int8)\n",
        "        head_mask = torch.cat((head_mask, head_mask_pad), dim=0)\n",
        "\n",
        "        mask_agg = torch.cat((head_mask.unsqueeze(0), mask.unsqueeze(0)), dim=0).flatten()\n",
        "\n",
        "        output_dict = {\"input_ids\":x_tensor_agg, \"attention_mask\": mask_agg}\n",
        "        # print(x_encoded_agg)\n",
        "        \n",
        "        if self.labels is not None:\n",
        "            y = self.labels[index]\n",
        "            y_encoded = torch.Tensor([self.label_dict.get(y, -1)]).long().squeeze(0)\n",
        "            # print(y_encoded)\n",
        "            # x_encoded[\"targets\"] = y_encoded\n",
        "\n",
        "        return output_dict, y_encoded\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZdUdWdx0sac"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'distilbert-base-uncased' # pretrained model from Transformers\n",
        "LOG_DIR = \"./logdir\"                   # for training logs and tensorboard visualizations\n",
        "NUM_EPOCHS = 3                         # smth around 2-6 epochs is typically fine when finetuning transformers\n",
        "BATCH_SIZE = 32                        # depends on your available GPU memory (in combination with max seq length)\n",
        "MAX_SEQ_LENGTH = 128                   # depends on your available GPU memory (in combination with batch size)\n",
        "NUM_CLASSES = 2                        # solving 3-class classification problem\n",
        "LEARN_RATE = 5e-5                      # learning rate is typically ~1e-5 for transformers\n",
        "ACCUM_STEPS = 4                        # one optimization step for that many backward passes\n",
        "SEED = 17\n",
        "\n",
        "train_dataset = ClickbaitDataset(texts = train[\"text\"], \n",
        "                                 headlines = train[\"title\"], \n",
        "                                 labels=train['label'], \n",
        "                                 label_dict=None, \n",
        "                                 max_seq_length=MAX_SEQ_LENGTH)\n",
        "valid_dataset = ClickbaitDataset(texts = valid[\"text\"], \n",
        "                                 headlines = valid[\"title\"], \n",
        "                                 labels=valid['label'], \n",
        "                                 label_dict=train_dataset.label_dict,\n",
        "                                 max_seq_length=MAX_SEQ_LENGTH)\n",
        "test_dataset = ClickbaitDataset(texts = test[\"text\"], \n",
        "                                headlines = test[\"title\"], \n",
        "                                labels=None, \n",
        "                                label_dict=None,\n",
        "                                max_seq_length=MAX_SEQ_LENGTH)\n",
        "\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_dataset,\n",
        "                        batch_size=BATCH_SIZE, \n",
        "                        shuffle=True)\n",
        "\n",
        "\n",
        "valid_dataloader = DataLoader(dataset=valid_dataset,\n",
        "                        batch_size=BATCH_SIZE, \n",
        "                        shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3dH6J97IYce",
        "outputId": "d8a74863-52b7-45f5-8b97-8c18278c383c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "({'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int8),\n",
              "  'input_ids': tensor([  101,  1002,  6365,  1012,  1018,  2213,  1002, 20003,  2213,  1002,\n",
              "          11176,  1012,  1023,  2213,  1002,  5539,  2213,   102,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,   101,  1002,\n",
              "           6365,  1012,  1018,  2213,  1002, 20003,  2213,  1002, 11176,  1012,\n",
              "           1023,  2213,  1002,  5539,  2213,   102,  1999,  1996,  2200,  2190,\n",
              "          11028,  3441,  1010,  1996,  9543,  5484,  2121, 15970,  2086,  2000,\n",
              "          16281,  1999, 27885, 28817, 15780,  2077,  1996,  2502, 12687,  1012,\n",
              "           7673,  2079,  6784,  2532,  1521,  1055,  2466,  2003,  2066,  2008,\n",
              "           1012,  1996,  4720,  1011,  2095,  1011,  2214, 16012, 24229,  3473,\n",
              "           2039,  1999,  7359,  1010,  3273,  2012, 13433, 21781,  2267,  1998,\n",
              "           5765,  1010,  1998,  2101,  2587,  1996,  4513,  2012,  1996,  2118,\n",
              "           1997,  2662,  2012,  8256,  1012,  2096,  1996,  2717,  1997,  1996,\n",
              "           4045,  2088,  2790,  8081,  4383,  2006,  6064,  1010,  1996,  2630,\n",
              "          16550,  1997,  2035,  7366,  1010,  2079,  6784,  2532,  2328,  6851,\n",
              "           7341,  1997, 12987,  1010,  2029,  2018,  2146,  2042,  2245,  1997,\n",
              "           2004,  1996,  8884,  3329,  5268,   102])},\n",
              " tensor(1))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-F0wkSm00tm"
      },
      "outputs": [],
      "source": [
        "class ClickbaitDetector(nn.Module):\n",
        "    def __init__(self, model_name, num_classes=None):\n",
        "        super(ClickbaitDetector, self).__init__()\n",
        "\n",
        "        config = AutoConfig.from_pretrained(model_name)\n",
        "\n",
        "        self.model = AutoModel.from_pretrained(model_name, config=config)\n",
        "        self.fclayer = nn.Linear(config.dim, config.dim)\n",
        "        self.classifier = nn.Linear(config.dim, num_classes)\n",
        "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, head_mask=None):\n",
        "        assert attention_mask is not None, \"attention mask is none\"\n",
        "        \n",
        "        model_output = self.model(input_ids=input_ids, attention_mask=attention_mask, head_mask=head_mask)\n",
        "\n",
        "        hidden_state = model_output[0]\n",
        "        pooled_output = hidden_state[:, 0]\n",
        "        pooled_output = self.fclayer(pooled_output)\n",
        "        pooled_output = nn.Sigmoid()(pooled_output)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "IC4OjiOUHSoH",
        "outputId": "9b312c66-093f-447d-d74a-9606ece8adb6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-305f378b4a3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClickbaitDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARN_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ClickbaitDetector' is not defined"
          ]
        }
      ],
      "source": [
        "model = ClickbaitDetector(model_name=MODEL_NAME, num_classes=NUM_CLASSES).cuda()\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss().cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARN_RATE)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLGLEmSQJMtm"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "    running_loss = 0.\n",
        "    total_acc_train = 0.\n",
        "    last_loss = 0.\n",
        "    \n",
        "\n",
        "    for i, data in tqdm(enumerate(train_dataloader)):\n",
        "\n",
        "        inputs, labels = data\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        \n",
        "        # Zero your gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        # outputs = model(inputs, masks)\n",
        "        outputs = model(inputs['input_ids'].cuda(), inputs['attention_mask'].cuda())\n",
        "\n",
        "        # Compute the accuracy\n",
        "        acc = (outputs.argmax(dim=1)==labels).sum().item()/labels.shape[0]\n",
        "        total_acc_train += acc\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            last_loss = running_loss / 100 # loss per batch\n",
        "            last_acc = total_acc_train / 100 # acc per batch\n",
        "            print('  batch {} loss: {} | acc: {}'.format(i + 1, last_loss, last_acc))\n",
        "            tb_x = epoch_index * len(train_dataset) + i + 1\n",
        "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "            running_loss = 0.\n",
        "            total_acc_train = 0.\n",
        "\n",
        "            # total_acc_val = 0.\n",
        "\n",
        "            # for j, valid_data in enumerate(valid_dataloader):\n",
        "            #     val_inputs, val_labels = valid_data\n",
        "            #     val_labels = val_labels.to(device)\n",
        "\n",
        "            #     with torch.no_grad():\n",
        "            #         val_outputs = model(val_inputs['input_ids'].cuda(), val_inputs['attention_mask'].cuda())\n",
        "            #         val_acc = (val_outputs.argmax(dim=1)==val_labels).sum().item()/val_labels.shape[0]\n",
        "            #         print(val_acc)\n",
        "            #         total_acc_val += val_acc\n",
        "\n",
        "            # print(f\"validation accuracy: {total_acc_val / j+1}\")        \n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "    return last_loss, last_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed7FdbVqNAe3",
        "outputId": "0fb461f7-37c7-4aec-ec08-62bd3a4c3339"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [01:14,  1.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  batch 100 loss: 0.5360058930516243 | acc: 0.783125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "200it [02:28,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  batch 200 loss: 0.5148235127329827 | acc: 0.7990625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "300it [03:42,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  batch 300 loss: 0.5056973880529404 | acc: 0.8053125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "400it [04:56,  1.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  batch 400 loss: 0.5078784504532814 | acc: 0.7996875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "500it [06:10,  1.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  batch 500 loss: 0.522824182510376 | acc: 0.791875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "573it [07:04,  1.35it/s]\n",
            "100it [01:14,  1.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  batch 100 loss: 0.4995529702305794 | acc: 0.8028125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "200it [02:27,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  batch 200 loss: 0.5211602938175202 | acc: 0.79125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "300it [03:41,  1.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  batch 300 loss: 0.5156631743907929 | acc: 0.7903125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "400it [04:55,  1.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  batch 400 loss: 0.49721648544073105 | acc: 0.8046875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "500it [06:09,  1.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  batch 500 loss: 0.506161393225193 | acc: 0.7965625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "573it [07:03,  1.35it/s]\n",
            "100it [01:13,  1.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  batch 100 loss: 0.4919824668765068 | acc: 0.8078125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "200it [02:27,  1.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  batch 200 loss: 0.5003746378421784 | acc: 0.8025\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "300it [03:41,  1.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  batch 300 loss: 0.5241887336969375 | acc: 0.7859375\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "400it [04:55,  1.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  batch 400 loss: 0.5067513501644134 | acc: 0.798125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "500it [06:09,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  batch 500 loss: 0.5164433017373085 | acc: 0.790625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "573it [07:03,  1.35it/s]\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter(f'run/detector_{timestamp}')\n",
        "\n",
        "for e in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    train_one_epoch(e, writer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsINqoNRffg8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Clickbait Detection.ipynb",
      "provenance": [],
      "mount_file_id": "1wnTif2WtMQeVgjhrbvMycpqTIXXlDx42",
      "authorship_tag": "ABX9TyOAk7AQv0nwMKJma8umzvmw",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}